{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Perception and Multimedia Computing Final Project\n",
    "## A Simple, Euclidean Distance-Based Music Recommendation System\n",
    "### Julia Wilkins, December 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import all necessary libraries and extra packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8e78fda87b20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAudio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtinytag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTinyTag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np, scipy as sp, matplotlib.pyplot as plt, matplotlib, librosa, math, os, glob, json, IPython\n",
    "from IPython.display import Audio\n",
    "from tinytag import TinyTag\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import your dataset of songs, preferably from iTunes, into the 'data' folder (I have kept a few sample songs in there). We concatenate these file paths into a list called 'file_list' that will be used throughout. \n",
    "\n",
    "Note: Obviously, the more songs you include in the dataset, the longer everything will take and the more CPU we are using. I recommend starting with no more than 40-50 songs, which will still take a long time to process. Try using 20-30 for a quicker result! Also, the year and genre features are extracted using the built in iTunes/finder metadata, so your results will be more accurate if your chosen songs have this info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_list = glob.glob(os.path.join('data', '*.mp3')) + glob.glob(os.path.join('data', '*.m4a'))+ glob.glob(os.path.join('data', '*.wav'))\n",
    "#print(file_list) # make sure everything is here so far!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature Extraction/Computation\n",
    "\n",
    "Next, we perform various feature calculations for each song, including BPM, RMS, spectral centroid, and MFCC average. We will use these features to calculate how similar each song is to every other song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculates approximate BPM of a song using Librosa's tempo estimator.\n",
    "def calculate_bpm(song_path):\n",
    "    y, sr = librosa.load(song_path, 44100)\n",
    "    onset_env = librosa.onset.onset_strength(y, sr=sr)\n",
    "    bpm = librosa.beat.estimate_tempo(onset_env, sr=sr)\n",
    "    return bpm\n",
    "\n",
    "\n",
    "# Calculates average spectral centroid over each song. \n",
    "# This gives us a decent measure of the overall brightness or timbre of a song.\n",
    "def calculate_spectral_centroid(song_path):\n",
    "    y, sr = librosa.load(song_path, )\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spectral_average = np.ndarray.mean(spectral_centroids)\n",
    "    return spectral_average\n",
    "\n",
    "\n",
    "# Calculate average Root Mean Squared (RMS) energy for each song.\n",
    "def calculate_av_RMS(song_path):\n",
    "    y, sr = librosa.load(song_path, 44100)\n",
    "    rms_array = librosa.feature.rmse(y=y)\n",
    "    rms_mean = np.ndarray.mean(rms_array)\n",
    "    return rms_mean\n",
    "\n",
    "\n",
    "# Calculate average Mel Frequency Cepstral Coefficients for each song.\n",
    "def calculate_MFCC(song_path):\n",
    "    y, sr = librosa.load(song_path, 44100)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)\n",
    "    return mfcc_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling our dictionaries\n",
    "\n",
    "Our metadata for each track will be stored in a nested dictionary of the form {song1_name : { feature1 : value,  feature2: value ... } ... songn : { feature1 : value ... }}. The outer dictionary contains the paths to each song as keys and an inner dictionary of features and associated results as values. The helper functions below fill each of these mini-dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def year_filler(file_list):\n",
    "    year_dict = {}\n",
    "    for file in file_list:\n",
    "        tag = TinyTag.get(file)\n",
    "        year = tag.year\n",
    "        #print(year)\n",
    "        if(year):\n",
    "            year = ''.join(list(year[:4]))\n",
    "            year_dict[file] = int(year) #sorry this is hacky\n",
    "        else:\n",
    "            year_dict[file] = None\n",
    "    return year_dict\n",
    "\n",
    "\n",
    "def genre_filler(file_list):\n",
    "    genre_dict = {}\n",
    "    for file in file_list:\n",
    "        tag = TinyTag.get(file)\n",
    "        genre = tag.genre\n",
    "        if(genre):\n",
    "            genre_dict[file] = str(genre)     \n",
    "        else:\n",
    "            genre_dict[file] = None\n",
    "    return genre_dict\n",
    "\n",
    "\n",
    "def bpm_filler(file_list):\n",
    "    bpm_dict = {}\n",
    "    for file in file_list:\n",
    "        bpm = calculate_bpm(file)\n",
    "        bpm_dict[file] = float(bpm)\n",
    "    return bpm_dict\n",
    "\n",
    "\n",
    "def RMS_filler(file_list):\n",
    "    RMS_dict = {}\n",
    "    for file in file_list:\n",
    "        rms = calculate_av_RMS(file)\n",
    "        RMS_dict[file] = float(rms)     \n",
    "    return RMS_dict\n",
    "\n",
    "\n",
    "def MFCC_filler(file_list):\n",
    "    MFCC_dict = {}\n",
    "    for file in file_list:\n",
    "        MFCC = calculate_MFCC(file)\n",
    "        MFCC_dict[file] = MFCC    \n",
    "    return MFCC_dict\n",
    "\n",
    "\n",
    "def spec_filler(file_list):\n",
    "    spec_dict = {}\n",
    "    for file in file_list:\n",
    "        spec = calculate_spectral_centroid(file)\n",
    "        spec_dict[file] = float(spec)     \n",
    "    return spec_dict\n",
    "\n",
    "#bpm_filler(file_list)\n",
    "#year_filler(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little helper function so populating the nest dictionaries is easier in fill_big:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calls our filler functions to populate the main dictionary. Use this above in fill_big.\n",
    "def fill_dicts(meta_dict, info_dict, secondary_key):\n",
    "    for key in info_dict:\n",
    "        meta_dict[key][secondary_key] = info_dict[key]\n",
    "    return meta_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we populate a large nested dictionary (meta_dict) of the format song_name[feature][data] This performs the heavy lifting of calling the functions that have to process the audio so it can take a while if you have a lot of files.\n",
    "\n",
    "Note: I have experienced a strange error on occasion when attempting to access the Year of some files. I have not resolved how to get around this error yet, and would recommend printing out each file in year_filler to see which file the function is failing on, and simply removing that track from your dataset. More fixed to come soon!\n",
    "\n",
    "**BE PATIENT!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fill_big(file_list):\n",
    "\n",
    "    meta_dict = {}\n",
    "    \n",
    "    for item in file_list:\n",
    "        meta_dict[item] = {\n",
    "            'Year': None,\n",
    "            'Genre': None,\n",
    "            'BPM': None,\n",
    "            'Spectral Centroid Average': None,\n",
    "            'RMS Average': None,\n",
    "            'MFCC Average': None,\n",
    "    }\n",
    "        \n",
    "    years = year_filler(file_list)\n",
    "    meta_dict = fill_dicts(meta_dict, years, 'Year')\n",
    "    \n",
    "    genres = genre_filler(file_list)\n",
    "    meta_dict = fill_dicts(meta_dict, genres, 'Genre')\n",
    "\n",
    "    bpms = bpm_filler(file_list)\n",
    "    meta_dict = fill_dicts(meta_dict, bpms, 'BPM')\n",
    "    \n",
    "    RMS = RMS_filler(file_list)\n",
    "    meta_dict = fill_dicts(meta_dict, RMS, 'RMS Average')\n",
    "    \n",
    "    mfcc = MFCC_filler(file_list)\n",
    "    meta_dict = fill_dicts(meta_dict, mfcc, 'MFCC Average')\n",
    "    \n",
    "    spec = spec_filler(file_list)\n",
    "    meta_dict = fill_dicts(meta_dict, spec, 'Spectral Centroid Average')\n",
    "    \n",
    "    return meta_dict\n",
    " \n",
    "# Save this result as meta_dict, to be used later.\n",
    "meta_dict = fill_big(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment this if you want to see what your metadata dictionary looks like! Helpful for understanding how the rest of the calculations are happening.\n",
    "#print(meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use our meta_dict dictionary to calculate the distance between each feature. track_distance, which calculates the average similarity over a number of features between any two tracks will be called in sim_filler, as we see below.\n",
    "\n",
    "As we aren't yet using this data and the distance values for classification/ML of any sort, I did not scale them to output a final similarity value between 0 and 1. I am simply summing the distances between the features of each track (with some objective weighting as you can see in track_distance), and using the tracks with the lowests distances as the most similar tracks.\n",
    "\n",
    "There is still work to be done here to improve the weighting/scaling of features. The next step is normalizing each feature to 0 to 1 and then take a weighted average so that our similarity value is between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computes total distance beatween all features given 2 tracks.\n",
    "def track_distance(track1, track2, meta_dict):\n",
    "    \n",
    "    if(meta_dict[track1][\"Year\"] == None):\n",
    "        year_dist = 5 # this logic is flawed but not sure how to deal with one file not having year data yet...\n",
    "    elif(meta_dict[track1][\"Year\"] != None and meta_dict[track2][\"Year\"] == None):\n",
    "        year_dist = 5\n",
    "    else:  \n",
    "        year_dist = np.abs(meta_dict[track1][\"Year\"] - meta_dict[track2][\"Year\"])\n",
    "        \n",
    "    bpm_dist = np.abs(meta_dict[track1]['BPM'] - meta_dict[track2]['BPM'])\n",
    "    rms_dist = np.abs(meta_dict[track1]['RMS Average'] - meta_dict[track2]['RMS Average'])\n",
    "    spec_dist = np.abs(meta_dict[track1]['Spectral Centroid Average'] - meta_dict[track2]['Spectral Centroid Average'])\n",
    "    \n",
    "    #using scipy built in euclidean distance function \n",
    "    mfcc_dist = distance.euclidean(meta_dict[track1]['MFCC Average'], meta_dict[track2]['MFCC Average'])\n",
    "    \n",
    "    if(meta_dict[track1]['Genre'] == meta_dict[track2]['Genre']):\n",
    "        genre_dist = 0 #we want a lower outcome if the genre matches\n",
    "    else:\n",
    "        genre_dist = 1\n",
    "    \n",
    "    # weight some of these depending on metric\n",
    "    total_dist = year_dist + bpm_dist + (rms_dist * 5) + (spec_dist * 0.05) + mfcc_dist + (genre_dist * 15)\n",
    "    return total_dist\n",
    " \n",
    "# Example:    \n",
    "#track_distance('data/02 Bodyache.m4a', 'data/01 Heartsigh.m4a', meta_dict)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill a mini similarity dictionary with keys = song names, values = total similarity distance to original track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Makes a similarity dictionary relative to the original file.\n",
    "def sim_filler(original, file_list):\n",
    "    sim_dict = {}\n",
    "    for file in file_list:\n",
    "        sim = track_distance(original, file, meta_dict)\n",
    "        sim_dict[file] = float(sim)  \n",
    "    return sim_dict\n",
    "\n",
    "\n",
    "# Replace the original field with the song you want to get recommendations relative to.\n",
    "# This song must be one of the songs in your original dataset.\n",
    "\n",
    "sim_dict = sim_filler('data/02 Bodyache.m4a', file_list )\n",
    "#print(sim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sorts our similarity dictionary (returns a tuple array not a dict because sorting dicts is impossible)\n",
    "def sort_sim_dict(sim_dict):\n",
    "    sorted_dict = sorted(sim_dict.items(), key=lambda x: x[1])\n",
    "    #print(sorted_dict)\n",
    "    just_names = [x[:-1] for x in sorted_dict]\n",
    "    return list(just_names)\n",
    "\n",
    "result = sort_sim_dict(sim_dict)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listen to your results!\n",
    "Now that we have our similarity values relative to each track, we can sort these distances in ascending order (smallest distance => closest features to original) and then return however many top recommended tracks you want to hear. Listen to them below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Listen back to your top n-1 tracks (the first will be your original, so n=3 if you want top 2 most similar. )\n",
    "# YAY! This is fun.\n",
    "def player(result, n):\n",
    "\n",
    "    for i in range(0, n):\n",
    "        element = result[i]\n",
    "        element = str(element)\n",
    "        element = list(element)\n",
    "        element = element[2:len(element)-3]\n",
    "        element = ''.join(element)\n",
    "        if(i==0):\n",
    "            print('Your original track was:')\n",
    "        elif(i==1):\n",
    "            print('This is your most recommended song!')\n",
    "        elif(i==2):\n",
    "            print('This is your',i,'nd most recommended song!')\n",
    "        elif(i==3):\n",
    "            print('This is your',i,'rd most recommended song!')\n",
    "        else:\n",
    "            print('This is your',i,'th most recommended song!')\n",
    "            \n",
    "        print(element[5:])\n",
    "        \n",
    "        song, sr = librosa.load(element, 44100 )\n",
    "        IPython.display.display(Audio(song, rate=sr))\n",
    "        \n",
    "# Uncomment to see (and hear(!)) the magic happen!\n",
    "#player(result, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "563d60528c70c9d2f8be224a4f4047a11e373d7a802564375b2bf5b7ea4d9513"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}